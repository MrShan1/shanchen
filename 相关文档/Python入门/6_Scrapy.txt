# 介绍
Scrapy是一个为了爬取网站数据，提取结构性数据而编写的应用框架。 可以应用在包括数据挖掘，信息处理或存储历史数据等一系列的程序中。Scrapy 使用 Twisted这个异步网络库来处理网络通讯，架构清晰，并且包含了各种中间件接口，可以灵活的完成各种需求。



# 整体架构及数据流向
引擎(Scrapy Engine)，用来处理整个系统的数据流处理，触发事务。

调度器(Scheduler)，用来接受引擎发过来的请求，压入队列中，并在引擎再次请求的时候返回。

下载器(Downloader)，用于下载网页内容，并将网页内容返回给蜘蛛。

蜘蛛(Spiders)，蜘蛛是主要干活的，用它来制订特定域名或网页的解析规则。编写用于分析response并提取item(即获取到的item)或额外跟进的URL的类。 每个spider负责处理一个特定(或一些)网站。

项目管道(Item Pipeline)，负责处理有蜘蛛从网页中抽取的项目，他的主要任务是清晰、验证和存储数据。当页面被蜘蛛解析后，将被发送到项目管道，并经过几个特定的次序处理数据。

下载器中间件(Downloader Middlewares)，位于Scrapy引擎和下载器之间的钩子框架，主要是处理Scrapy引擎与下载器之间的请求及响应。

蜘蛛中间件(Spider Middlewares)，介于Scrapy引擎和蜘蛛之间的钩子框架，主要工作是处理蜘蛛的响应输入和请求输出。

调度中间件(Scheduler Middlewares)，介于Scrapy引擎和调度之间的中间件，从Scrapy引擎发送到调度的请求和响应。



绿线是数据流向，数据流向由执行引擎(Scrapy Engine)控制

首先从初始URL开始，调度器(Scheduler)会将其交给下载器(Downloader)进行下载，下载之后会交给蜘蛛(Spiders)进行分析，
蜘蛛(Spiders)分析出来的结果有两种：
一种是需要进一步抓取的链接，例如之前分析的“下一页”的链接，这些东西会被传回调度器(Scheduler)；
另一种是需要保存的数据，它们则被送到项目管道(Item Pipeline)那里，那是对数据进行后期处理（详细分析、过滤、存储等）的地方。

另外，在数据流动的通道里还可以安装各种中间件，进行必要的处理。


# 演示代码
exit() # 退出Python交互环境
D:
scrapy startproject myproject    # 创建一个Scrapy项目
cd myproject
scrapy genspider mydemo www.163.com  # 创建一个新的spider
scrapy crawl mydemo -o items.json

# 有些网站会在根目录下放置一个名字为robots.txt的文件，里面声明了此网站希望爬虫遵守的规范
# 如果把ROBOTSTXT_OBEY设置成了 True，Scrapy就会遵守这个文件制定的规范
ROBOTSTXT_OBEY = False
# 修改请求头
USER_AGENT = 'Mozilla/5.0 (Windows NT 10.0; WOW64; rv:48.0) Gecko/20100101 Firefox/48.0'
# 设置下载的等待时间(0.5到1.5之间的一个随机值 * DOWNLOAD_DELAY)
# 下载等待时间长，不能满足段时间大规模抓取的要求，太短则大大增加了被ban的几率
RANDOMIZE_DOWNLOAD_DELAY = True
DOWNLOAD_DELAY=0.5
# 禁止cookies，也就是不启用cookies middleware，不想web server发送cookies。
# 所谓cookies，是指某些网站为了辨别用户身份而储存在用户本地终端（Client Side）上的数据（通常经过加密），
# 禁止cookies也就防止了可能使用cookies识别爬虫轨迹的网站得逞。
COOKIES_ENABLED = False


import scrapy

class DemoItem(scrapy.Item):
    # 对item进行建模
    link = scrapy.Field()
    title = scrapy.Field()
    published = scrapy.Field()
    origin = scrapy.Field()



import re
from myproject.items import DemoItem

    def parse(self, response):
        for url in response.xpath('//a/@href').extract():
            if re.match(r'.*?163.com/\d{2}/\d{4}/\d{2}/.*?.html', url):
                yield scrapy.Request(url=url, callback=self.parse_item)
        pass

    def parse_item(self, response):
        url = response.url
        flag = True
        item = DemoItem()
        item['link'] = url
        ext = response.xpath('//h1/text()').extract()
        if ext:
            item['title'] = ext[0]
        else:
            flag = False
        ext = response.xpath('//div[@class="post_time_source"]').extract()
        if ext:
            time_source = ext[0]
            search_p = re.search(r'\d{4}-\d{1,2}-\d{1,2}( \d{1,2}:\d{1,2}(:\d{1,2})?)?', time_source)
            search_o = re.search(ur'来源:(.*)', time_source)
            item['published'] = search_p.group(0) if search_p else None
            item['origin'] = re.sub(r'<.*?>', '', search_o.group(1)).strip() if search_o else None
        if flag:
            yield item
        else:
            print 'Error:' + url
        pass